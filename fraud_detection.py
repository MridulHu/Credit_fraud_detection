# -*- coding: utf-8 -*-
"""fraud-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oka8st7SdAUZxnfWWt9QPxMH2-3TRTnV
"""

import torch
import seaborn as sns
import matplotlib.pyplot as plt
device = "cuda" if torch.cuda.is_available() else "cpu"
device

import pandas as pd
import kagglehub

# Download dataset
path = kagglehub.dataset_download("kartik2112/fraud-detection")

# Read CSV files dynamically
te_data = pd.read_csv(f"{path}/fraudTest.csv")
tr_data = pd.read_csv(f"{path}/fraudTrain.csv")

# Combine datasets
data = pd.concat([te_data, tr_data])

print("Dataset loaded successfully!")

fraud_data = data[data['is_fraud'] == 1]
len(fraud_data)

non_fraud_data = data[data['is_fraud'] == 0]
len(non_fraud_data)

test_fraud_data = fraud_data[:4000]
test_non_fraud_data = non_fraud_data[:4000]

val_fraud_data = fraud_data[4000:6000]
val_non_fraud_data = non_fraud_data[4000:6000]

train_fraud_data = fraud_data[6000:]
train_non_fraud_data = non_fraud_data[6000:]

test_data = pd.concat([test_fraud_data,test_non_fraud_data])
val_data = pd.concat([val_fraud_data,val_non_fraud_data])
train_data = pd.concat([train_fraud_data,train_non_fraud_data])

len(test_data), len(val_data), len(train_data)

import torch
import torch.nn as nn
import torch.optim as optim
import joblib

class AutoencoderModel(nn.Module):
    def __init__(self, input_size: int = 22, bottleneck_size: int = 7):
        super(AutoencoderModel, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, bottleneck_size),
        )
        self.decoder = nn.Sequential(
            nn.Linear(bottleneck_size, 64),
            nn.ReLU(),
            nn.Linear(64, input_size),
            nn.Sigmoid()  # Using Sigmoid for output to keep it between 0 and 1
        )

    def forward(self, x):
        encoded = self.encoder(x)
        reconstructed = self.decoder(encoded)
        return reconstructed

    def fit(self, X: torch.Tensor, num_epochs: int = 50, learning_rate: float = 0.001):
        # Set the model to training mode
        self.train()

        # Loss function and optimizer
        criterion = nn.MSELoss()  # Mean Squared Error for reconstruction
        optimizer = optim.Adam(self.parameters(), lr=learning_rate)

        for epoch in range(num_epochs):
            optimizer.zero_grad()

            # Forward pass
            reconstructed = self(X)

            # Calculate loss
            loss = criterion(reconstructed, X)
            loss.backward()
            optimizer.step()

            print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}")

    def transform(self, X: torch.Tensor):
        # Set the model to evaluation mode
        self.eval()

        with torch.no_grad():
            encoded = self.encoder(X)  # Only return the encoded representation
        return encoded

    def save(self, filepath: str):
        joblib.dump(self, filepath)
        print(f"Model saved to {filepath}")

    @staticmethod
    def load(filepath: str):
        model = joblib.load(filepath)
        print(f"Model loaded from {filepath}")
        return model

import pandas as pd
import torch
import datetime
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import WeightedRandomSampler, DataLoader, TensorDataset, Subset
import random
import seaborn as sns
import joblib
from imblearn.over_sampling import SMOTE
import os

class DataProcessor():
    def __init__(self, scaler_path: str = "./scaler.pkl", autoencoder_path: str = "autoencoder_model.pkl"):
        self.scaler_path = scaler_path
        self.autoencoder_path = autoencoder_path

    def feature_selection(self, df: pd.DataFrame):
        drop_columns = ['cc_num','merchant','unix_time','merch_lat','merch_long','trans_year']
        df = df.drop(columns=drop_columns)
        return df

    def format_data(self, data: pd.DataFrame) -> tuple:
        # Convert the column names to lower case and replace spaces with underscores
        data.columns = data.columns.str.replace(' ', '_').str.lower()
        data = data.dropna()

        # Convert the date columns to datetime
        data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])
        data['dob'] = pd.to_datetime(data['dob'])

        # Extract the year, month, day, hour and age from the date columns
        data['trans_year'] = data['trans_date_trans_time'].dt.year
        data['trans_month'] = data['trans_date_trans_time'].dt.month
        data['trans_day'] = data['trans_date_trans_time'].dt.day
        data['trans_hour'] = data['trans_date_trans_time'].dt.hour

        # Get the current year and calculate the age
        current_year = datetime.datetime.now().year
        data['age'] = current_year - data['dob'].dt.year

        # Drop unnecessary columns
        data = data.drop(columns=['trans_num', 'trans_date_trans_time', 'first', 'last', 'dob', 'unnamed:_0'])

        # Encode the string columns
        object_cols = data.select_dtypes(include=['object']).columns
        label_encoder = LabelEncoder()

        # Apply label encoding to string columns
        for col in object_cols:
            data[col] = label_encoder.fit_transform(data[col])

        return data

    def get_x_y(self, data: pd.DataFrame) -> tuple:
        X = torch.tensor(data.values,dtype=torch.float)

        try:
            y = torch.tensor(data['is_fraud'].values,dtype=torch.long)
        except:
            y = None

        return X, y

    def check_for_imbalance(self, data: pd.DataFrame) -> tuple:
        data.columns = data.columns.str.replace(' ', '_').str.lower()
        num_class_0 = data[data['is_fraud'] == 0].shape[0]
        num_class_1 = data[data['is_fraud'] == 1].shape[0]

        data = [num_class_1, num_class_0]
        keys = ['Is Fraud', 'Not Fraud']

        # define Seaborn color palette to use
        palette_color = sns.color_palette('bright')

        # plotting data on chart
        plt.pie(data, labels=keys, colors=palette_color, autopct='%.0f%%')

        # displaying chart
        plt.show()

        print(f"Non-Fraud data count: {num_class_0}")
        print(f"Fraud data count: {num_class_1}")
        return num_class_0, num_class_1

    def apply_sampler(self, X: torch.Tensor, y: torch.Tensor, num_class_0: int, num_class_1: int) -> tuple:
        # Create weights for each sample
        sample_weights = [1.0 / num_class_0 if label == 0 else 1.0 / num_class_1 for label in y]

        # Create the sampler
        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)
        dataset = TensorDataset(X, y)

        # Create DataLoader with the sampler
        dataloader = DataLoader(dataset, sampler=sampler, batch_size=len(y))  # Use the entire dataset

        # Count classes
        class_counts = {0: 0, 1: 0}
        valid_inputs_list = []  # To store inputs
        valid_labels_list = []  # To store labels

        for inputs, labels in dataloader:
            valid_inputs_list.append(inputs)
            valid_labels_list.append(labels)

            # Count occurrences of each class
            class_counts[0] += (labels == 0).sum().item()
            class_counts[1] += (labels == 1).sum().item()

        print(f'Class distribution: {class_counts}')

        # Create newX and newY from the concatenated lists
        newX = torch.cat(valid_inputs_list)  # Concatenate tensors
        newY = torch.cat(valid_labels_list)  # Concatenate tensors

        return newX, newY

    def apply_smote(self, X: torch.Tensor, y: torch.Tensor) -> tuple:
        # Convert torch Tensors to NumPy arrays, as SMOTE works with NumPy
        X_np = X.cpu().numpy()
        y_np = y.cpu().numpy()

        # Apply SMOTE to generate synthetic samples for the minority class
        smote = SMOTE(sampling_strategy='auto', random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X_np, y_np)

        # Convert back to torch Tensors
        newX = torch.tensor(X_resampled, dtype=torch.float32).to(X.device)
        newY = torch.tensor(y_resampled, dtype=torch.float32).to(y.device)

        # Count classes after SMOTE
        class_counts = {0: (newY == 0).sum().item(), 1: (newY == 1).sum().item()}
        print(f'Class distribution after SMOTE: {class_counts}')

        return newX, newY

    def split(self, X: torch.Tensor, y: torch.Tensor, val_size: int, test_size: int) -> tuple:
        # Check if the percentages are valid
        if val_size + test_size >= 100:
            raise ValueError("Sum of validation and test sizes must be less than 100.")

        train_size = 100 - val_size - test_size

        # Combine X and y into a single dataset
        full_dataset = TensorDataset(X, y)
        num_samples = len(full_dataset)

        # Shuffle the indices for randomness
        indices = list(range(num_samples))
        random.shuffle(indices)

        # Calculate split indices
        train_size = int(num_samples * (train_size / 100))
        val_size = int(num_samples * (val_size / 100))
        test_size = num_samples - train_size - val_size  # Ensure the remaining samples go to test

        # Create subsets
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        test_indices = indices[train_size + val_size:]

        # Create subsets
        train_subset = Subset(full_dataset, train_indices)
        val_subset = Subset(full_dataset, val_indices)
        test_subset = Subset(full_dataset, test_indices)

        # Split the subsets into X and y
        X_train, y_train = zip(*[train_subset[i] for i in range(len(train_subset))])
        X_val, y_val = zip(*[val_subset[i] for i in range(len(val_subset))])
        X_test, y_test = zip(*[test_subset[i] for i in range(len(test_subset))])

        # Convert back to tensors
        X_train, y_train = torch.stack(X_train), torch.stack(y_train)
        X_val, y_val = torch.stack(X_val), torch.stack(y_val)
        X_test, y_test = torch.stack(X_test), torch.stack(y_test)

        return X_train, y_train, X_val, y_val, X_test, y_test

    def apply_feature_scaling(self, X_train: torch.Tensor, X_val: torch.Tensor, X_test: torch.Tensor) -> tuple:
        # check if the scaler exists
        if os.path.exists(self.scaler_path):
            scaler = joblib.load(self.scaler_path)
        else:
            scaler = MinMaxScaler()

            # Fit the scaler on the training data
            scaler.fit(X_train.numpy())  # Fit the scaler using NumPy format of training data

            # Save the scaler
            joblib.dump(scaler, self.scaler_path)

        # Scale the training data
        scaled_X_train = torch.tensor(scaler.transform(X_train.numpy()), dtype=torch.float)

        # Scale the validation data
        scaled_X_val = torch.tensor(scaler.transform(X_val.numpy()), dtype=torch.float)

        # Scale the test data
        scaled_X_test = torch.tensor(scaler.transform(X_test.numpy()), dtype=torch.float)

        # Debugging: check shapes of the scaled data
        print(f"Scaled X_train shape: {scaled_X_train.shape}")
        print(f"Scaled X_val shape: {scaled_X_val.shape}")
        print(f"Scaled X_test shape: {scaled_X_test.shape}")

        return scaled_X_train, scaled_X_val, scaled_X_test

    def apply_encode(self, X_train: torch.Tensor, X_val: torch.Tensor, X_test: torch.Tensor, features: int, epoch: int = 100, rl: float = 0.001) -> tuple:
        # check if the autoencoder exists
        if os.path.exists(self.autoencoder_path):
            autoencoder = joblib.load(self.autoencoder_path)
        else:
            autoencoder = AutoencoderModel(input_size=X_train.shape[1], bottleneck_size=features)
            autoencoder.fit(X_train, num_epochs=epoch, learning_rate=rl)
            # Save the model
            joblib.dump(autoencoder, self.autoencoder_path)

        # Transform the data
        encoded_X_train = autoencoder.transform(X_train)
        encoded_X_val = autoencoder.transform(X_val)
        encoded_X_test = autoencoder.transform(X_test)

        return encoded_X_train, encoded_X_val, encoded_X_test

    def create_data_loaders(
        self,
        X_train: torch.Tensor,
        y_train: torch.Tensor,
        X_val: torch.Tensor,
        y_val: torch.Tensor,
        X_test: torch.Tensor,
        y_test: torch.Tensor,
        batch_size: int
    ) -> tuple:
        # Create TensorDataset for each set
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        test_dataset = TensorDataset(X_test, y_test)

        # Create DataLoaders for each set
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, val_loader, test_loader

    def plot_class_distribution(self, X: torch.Tensor, y: torch.Tensor):
        # Convert y tensor to NumPy array for counting
        y_np = y.cpu().numpy()  # Move to CPU and convert to NumPy

        # convert y_np to int
        y_np = y_np.astype(int)

        # Count occurrences of each class
        class_counts = np.bincount(y_np)

        data = [class_counts[1], class_counts[0]]
        keys = ['Is Fraud', 'Not Fraud']

        # define Seaborn color palette to use
        palette_color = sns.color_palette('bright')

        # plotting data on chart
        plt.pie(data, labels=keys, colors=palette_color, autopct='%.0f%%')

        # displaying chart
        plt.show()

    def preprocess(self, X: torch.Tensor) -> torch.Tensor:
        # Apply feature scaling
        scaler = joblib.load(self.scaler_path)
        scaled_X = torch.tensor(scaler.transform(X.numpy()), dtype=torch.float)

        # Apply Encode
        autoencoder = joblib.load(self.autoencoder_path)
        encoded_X = autoencoder.transform(scaled_X)

        return encoded_X

data_processor = DataProcessor(
    scaler_path="/content/scaler.pkl",
    autoencoder_path="/content/autoencoder.pkl"
)

test_data = data_processor.format_data(test_data)
val_data = data_processor.format_data(val_data)
len(test_data), len(val_data)

train_data = data_processor.format_data(train_data)
train_data

plt.figure(figsize=(12, 10))
sns.heatmap(train_data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

test_data = data_processor.feature_selection(test_data)
val_data = data_processor.feature_selection(val_data)
train_data = data_processor.feature_selection(train_data)
train_data

# Format Dataframe to Tensor
X_train, y_train = data_processor.get_x_y(train_data)
X_test, y_test = data_processor.get_x_y(test_data)
X_val, y_val = data_processor.get_x_y(val_data)
X_train[:5], y_train[:5]

num_class_0, num_class_1 = data_processor.check_for_imbalance(train_data)

# Apply Data Sampling
X_train, y_train = data_processor.apply_smote(X_train, y_train)
len(X_train), len(y_train)

data_processor.plot_class_distribution(X_train, y_train)

data_processor.plot_class_distribution(X_val, y_val)

data_processor.plot_class_distribution(X_test, y_test)

# Feature Scaling
X_train, X_val, X_test = data_processor.apply_feature_scaling(X_train, X_val, X_test)
X_train[:5], X_val[:5], X_test[:5]

# Apply Encoder
X_train, X_val, X_test = data_processor.apply_encode(X_train, X_val, X_test, 7)
X_train[:5], X_val[:5], X_test[:5]

train_loader, val_loader, test_loader = data_processor.create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=16)
len(train_loader), len(val_loader), len(test_loader)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, auc, precision_recall_curve
import pandas as pd

class Model(nn.Module):
    def __init__(self, input_size, model_path: str = None):
        super(Model, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )
        self.loss_fn = nn.BCEWithLogitsLoss()
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.0001)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True)

        if model_path:
            self.load_checkpoint(model_path)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)

    def set_loss_fn(self, loss_fn):
        self.loss_fn = loss_fn

    def set_optimizer(self, optimizer):
        self.optimizer = optimizer

    def set_scheduler(self, scheduler):
        self.scheduler = scheduler

    def save_checkpoint(self, filepath: str):
        torch.save({
            'model_state_dict': self.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }, filepath)
        print(f"Checkpoint saved to {filepath}")

    def load_checkpoint(self, filepath: str):
        checkpoint = torch.load(filepath, map_location=device)
        self.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"Checkpoint loaded from {filepath}")

    def train_model(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int = 10,
        patience: int = 5,
        checkpoint_path: str = 'model_checkpoint.pth',
        show_plot: bool = True
    ):
        # Transfer the model to GPU if available
        self.to(device)

        train_losses = []
        val_losses = []

        best_val_loss = float('inf')
        epochs_no_improve = 0  # Early stopping counter

        for epoch in range(num_epochs):
            self.train()  # Set the model to training mode
            epoch_loss = 0
            correct_predictions = 0  # Initialize correct predictions counter
            total_predictions = 0  # Initialize total predictions counter

            print(f"Epoch {epoch + 1}/{num_epochs}")

            for batch_idx, (inputs, targets) in enumerate(train_loader):
                inputs, targets = inputs.to(device), targets.to(device)
                logits = self(inputs)

                # Calculate loss
                loss = self.loss_fn(logits.squeeze(), targets.float())

                # Backward pass and optimizer step
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # Calculate accuracy
                y_pred_prob = torch.sigmoid(logits)  # Apply sigmoid to get probabilities
                y_pred_binary = (y_pred_prob >= 0.5).float()  # Threshold to get binary predictions
                correct_predictions += (y_pred_binary.view(-1) == targets).sum().item()  # Count correct predictions
                total_predictions += targets.size(0)  # Count total predictions

                epoch_loss += loss.item()

                if (batch_idx + 1) % 10000 == 0:
                    print(f"Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}")

            avg_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_loss)

            # Calculate and print training accuracy after the epoch
            train_accuracy = correct_predictions / total_predictions
            print(f"Avg Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")

            # Validation phase
            self.eval()  # Set the model to evaluation mode
            val_loss = 0
            val_correct_predictions = 0  # Initialize correct predictions for validation
            val_total_predictions = 0  # Initialize total predictions for validation

            with torch.no_grad():
                for val_inputs, val_targets in val_loader:
                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)
                    val_logits = self(val_inputs)
                    val_loss += self.loss_fn(val_logits.squeeze(), val_targets.float()).item()

                    # Calculate validation accuracy
                    val_y_pred_prob = torch.sigmoid(val_logits)  # Apply sigmoid to get probabilities
                    val_y_pred_binary = (val_y_pred_prob >= 0.5).float()  # Threshold to get binary predictions
                    val_correct_predictions += (val_y_pred_binary.view(-1) == val_targets).sum().item()  # Count correct predictions
                    val_total_predictions += val_targets.size(0)  # Count total predictions

            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)

            # Calculate and print validation accuracy after the epoch
            val_accuracy = val_correct_predictions / val_total_predictions
            print(f"Avg Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

            # Step the scheduler
            self.scheduler.step(avg_val_loss)

            # Early stopping check
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                epochs_no_improve = 0  # Reset counter
                # Save checkpoint when validation loss improves
                self.save_checkpoint(checkpoint_path)
            else:
                epochs_no_improve += 1

            if epochs_no_improve >= patience:
                print("Early stopping triggered!")
                break  # Stop training if no improvement

        if show_plot:
            # Plotting the training loss curve
            plt.figure(figsize=(10, 5))
            plt.plot(train_losses, label='Training Loss')
            plt.plot(val_losses, label='Validation Loss')
            plt.title('Loss Curve')
            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            plt.legend()
            plt.show()

    def test_model(self, test_loader: DataLoader,show_plot=True):
        self.to(device)

        self.eval()  # Set the model to evaluation mode
        test_loss = 0
        correct = 0
        total = 0
        y_true = []
        y_pred = []
        y_pred_prob = []

        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                logits = self(inputs)
                loss = self.loss_fn(logits.squeeze(), targets.float())
                test_loss += loss.item()

                # Collect predictions
                predicted = torch.sigmoid(logits) >= 0.5  # Apply sigmoid and threshold
                y_true.extend(targets.cpu().numpy())
                y_pred.extend(predicted.view(-1).cpu().numpy())
                y_pred_prob.extend(torch.sigmoid(logits).view(-1).cpu().numpy())

                correct += (predicted.view(-1).float() == targets).sum().item()
                total += targets.size(0)

        avg_test_loss = test_loss / len(test_loader)
        accuracy = correct / total

        print(f"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.4f}")

        # Evaluate metrics
        self.evaluate_metrics(y_true, y_pred, y_pred_prob, show_plot=show_plot)

    def evaluate_metrics(self, y_true, y_pred, y_pred_prob, show_plot=True):
        # Confusion matrix
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        print(f'Confusion Matrix:\n TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')

        # Precision, Recall, F1-Score
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)

        print(f'Precision: {precision:.4f}')
        print(f'Recall: {recall:.4f}')
        print(f'F1-Score: {f1:.4f}')

        # AUC-ROC
        auc_roc = roc_auc_score(y_true, y_pred_prob)
        print(f'AUC-ROC: {auc_roc:.4f}')

        # AUC-PR
        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_prob)
        auc_pr = auc(recall_vals, precision_vals)
        print(f'AUC-PR: {auc_pr:.4f}')

        # plot confusion matrix
        if show_plot:
            from sklearn.metrics import ConfusionMatrixDisplay
            cm = confusion_matrix(y_true, y_pred)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm)
            disp.plot()
            plt.show()

    def predict_tensor(self, X, threshold=0.5):
        logits = self(X)
        probabilities = torch.sigmoid(logits)
        predictions = (probabilities >= threshold).int()
        return predictions

    def predict_csv(self, file_path: str):
        # Load the data
        raw_data = pd.read_csv(file_path)

        # Preprocess the data
        processor = DataProcessor()
        X = processor.format_data(raw_data)
        X = processor.preprocess(X)

        with torch.no_grad():
            predictions = self.predict_tensor(X)

        return predictions

    def predict_df(self, df: pd.DataFrame):
        # Preprocess the data
        processor = DataProcessor()
        X = processor.format_data(df)
        X = processor.preprocess(X)

        with torch.no_grad():
            predictions = self.predict_tensor(X)

        return predictions

model = Model(input_size=7)
model

model.train_model(train_loader, val_loader, num_epochs=3, checkpoint_path="/content/model_checkpoint.pth")

model.test_model(test_loader)

